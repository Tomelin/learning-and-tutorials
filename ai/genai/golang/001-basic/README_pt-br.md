# üöÄ **Desvendando a IA com Golang e Google Gemini!** ‚ú®

Estou iniciando uma s√©rie pr√°tica onde exploro a integra√ß√£o de Golang com o Google Gemini. Abordarei desde a configura√ß√£o inicial at√© a constru√ß√£o de uma CLI para an√°lise de logs e eventos do Kubernetes usando o poder do Gemini.

Neste primeiro passo, vamos entender:

- A estrutura de projetos Go para LLMs.
- Como configurar o acesso √† API do Gemini (com foco no novo reposit√≥rio go-genai).
- A cria√ß√£o de um comando CLI simples para interagir com o Gemini, transformando eventos brutos em insights acion√°veis de SRE.

Prepare-se para ver como o Go pode ser uma ferramenta poderosa para construir solu√ß√µes de IA robustas e eficientes!

üí° Curioso para ver o c√≥digo e os detalhes t√©cnicos? Confere o reposit√≥rio/artigo completo [Link para o seu GitHub/Artigo Completo Aqui](https://github.com/Tomelin/learning-and-tutorials/blob/main/ai/genai/golang/001-basic)!

## Introdu√ß√£o
Nesse escopo iremos entender de forma b√°sica, como trabalhar com o Golang e o Google Gemini. A proposta √© ser pr√°tico.

Antes de come√ßarmos, no momento que escrevo esse post (MAI/2025), o Google Gemini est√° com dois reposit√≥rios no Github, sendo que um deles, foi considerado legado h√° pouco tempo:
https://github.com/google/generative-ai-go (legacy)
https://github.com/googleapis/go-genai (new)

Por√©m, diversos projetos est√£o usando o reposit√≥rio (legacy), o que muda um pouco os m√©todos e as chamadas, mas veremos os dois casos ao logo dessa s√©rie

## A estrutura de c√≥digo
Nesse artigo e nos pr√≥ximos irei usar a mesma estrutura de diret√≥rios e de c√≥digo, para facilitar o entendimento durante essa s√©rie.

Estrutura de diret√≥rios:
```
-cmd    #diret√≥rio com as inicializa√ß√µes do golang 
--cli   #inicializa o c√≥digo em cli (command line interface)
--rest  #inicializa o c√≥digo em http
-config #configura√ß√µes, que iremos ler do YAML
-internal  #c√≥digo restritio a essa app
--core  #regras de negocio da app
---entity      #a estrutura das nossas regras
---service     #regra de negocio
---repository  #tudo que for conex√£o com tipo de dados (db,mq,cache)
--handler @regras mais mais agrangencia, onde colocaremos http,mq, ...
-pkg ## c√≥digo compartilhado
--storage
---database
---mq
---cache
--llm
---gemini
```

Essa ser√° a estrutura b√°sica do nosso c√≥digo.  N√£o irei detalhar o arquivo config, apenas deixarei compartilhado, pois a unica coisa que ele faz, √© ler o arquivo YAML, arquivo que cont√©m as configura√ß√µes da nossa app


## Arquivo de configura√ß√£o
Nosso arquivo YAML, que ter√° as configura√ß√µes de token e demais servi√ßos, ter√° a seguinte estrutura inicial:
config.yaml
```
config:
  teste: "string"
genai:
  llm:
    gemini:
      api_key: {TOKEN}
      model: "gemini-2.0-flash"
webservice:
  port: 8080
datatase:
  user: root
  password: root
  host: localhost
```

Conforme avan√ßarmos, vamos ajustando os valores e param√™tro, pois nesse momento iremos usar apenas o token GEMINI

O link do config.go, [est√° aqui](https://github.com/Tomelin/learning-and-tutorials/blob/main/ai/genai/golang/001-basic/config/config.go).   O config verifica se existe a vari√°vel "PATH_CONFIG" que √© correspondente a onde se encontra o arquivo config.yaml.  Caso n√£o exista, ir√° procurar o arquivo config.yaml na raiz de onde est√° se executando o projeto.  Caso n√£o encontre em nenhum desse dois lugares, retornar√° erro

O config, ir√° retornar um map[string]interface das nossas configura√ß√µes e cada componente, ter√° que tratar as suas configs.

## Inicio do projeto
Nessa primeira etapa, iremos configurar o Golang com o Gemini, apenas para responder a uma request e na pr√≥xima etapa iremos configurar com sess√£o, para manter a conversa.

Tamb√©m configuraremos o cobra-cli.  Faremos algumas sess√µes da serie, implementando o HTTP.  Nessa primeiro, teremos o HTTP e CLI, para exemplificar o processo, nas pr√≥ximas, focaremos mais na CLI.

Ent√£o, nesse ponto, entendo que voc√™ j√° est√° com os diret√≥rios criados, conforme mencionado anteriormente.

iniciando o projeto em golang:
```
go mod init github.com/tomelin/learnai
```

Agora irei acessar o diret√≥rio cmd, para criar a cli
```
cd cmd
cobra-cli init learnai
mv learnai cli
```

Projeto iniciado!

## Criando o command do CLI
Vamos criar o comando chamado tshot, que vem de troubleshootig.

```
cobra-cli add tshot
```

ao lista o diret√≥rio CMD dentro de cli, teremos dois arquivos go:
```
ls  cmd/
root.go
tshot.go
```

Agora vamos colocar as flags necess√°rias no nosso troubleshooting, iremos usar as seguintes flags:

**question (q)** para alterar o valor padr√£o se necess√°rio  
**event (e)** o evento que iremos passar

Entendido os parametros, vamos criar os mesmos dentro do arquivo tshot.go na func init().

```
func init() {
	rootCmd.AddCommand(tshotCmd)

	// Cobra supports local flags which will only run when this command
  // Flag com objetivo de definir a Role do prompt a ser criado (opcional) ao executar a cli
	tshotCmd.Flags().StringP("question", "q", "", "Role: Act as an expert Kubernetes SRE Engineer. Focus on cluster health data analysis (Kubernetes, Cilium CNI, Ingress, Nginx, Karpenter, addons). Proactively identify, diagnose, and remediate issues and create a report and propose the solution")
  
  // Flag onde passaremos os logs ou eventos do kubernetes.  Lembra, teremos limites de caracteres , por causa da LLM
	tshotCmd.Flags().StringP("events", "e", "", "kubernetes events or logs")
}
```

Continuando no arquivo tshot.go, configuraremos a variable tshotCmd ficar√° da seguinte forma:
```
var tshotCmd = &cobra.Command{
	Use:   "tshot",
	Short: "Analyze events and logs from Kubernetes",
	Long: `Analyze events and logs from Kubernetes`,
	Run: func(cmd *cobra.Command, args []string) {
		log.SetFlags(log.LstdFlags | log.Lshortfile)

		query, err := cmd.Flags().GetString("query")
		if err != nil {
			log.Fatal(err.Error())
		}

		events, err := cmd.Flags().GetString("events")
		if err != nil {
			log.Fatal(err.Error())
		}

		configs, err := config.LoadConfig()
		if err != nil {
			log.Fatal(err.Error())
		}

		var genai gemini.GenAIConfig
		b, err := json.Marshal(configs.Fields["genai"])
		if err != nil {
			log.Fatal(err.Error())
		}

		err = json.Unmarshal(b, &genai)
		if err != nil {
			log.Fatal(err.Error())
		}

		if genai == (gemini.GenAIConfig{}) {
			log.Fatalln(errors.New("config cannot be empty"))
		}

		client, err := gemini.NewGeminiAgent(&genai, &query)
		if err != nil {
			log.Fatal(err.Error())
		}

		resp, err := client.Short(context.Background(), &events)
		if err != nil {
			log.Fatal(err.Error())
		}
		fmt.Println("************************************")
		fmt.Println("*************RESPONSE***************")
		fmt.Println(*resp)
	},
}
```

Definindo o import em tshot.go
```
import (
	"log"

	"github.com/spf13/cobra"
	"learn-ai/config"
	"learn-ai/pkg/llms/llm_agent"
	"learn-ai/pkg/llms/llm_gemini"
)
```

## Configurando a LLM Gemini
Vamos escrever o c√≥digo para que o Gemini, entenda o que estamos pesqusando.  Vamos criar o arquivo  **gemini.go** dentro do seguinte path **pkg/llm/gemini/gemini.go**

Esse arquivo deve ser um pouco extenso, mas ao decorrer da s√©rie, vamos complementando o mesmo, onde teremos alguns m√©todos.

Iniciando com os import, teremos os seguintes packages:
```
package gemini

import (
	"context"
	"errors"
	"fmt"
	"log"

	"github.com/google/generative-ai-go/genai"
	"google.golang.org/api/option"
)

```

Agora definiremos 2 struct e uma interface, conforme o que iremos desenvolvendo, deixarei os coment√°rios:
```
// Struct para definir a API KEY que vem do config.yaml e o Model da LLM que iremos utilizar
type Gemini struct {
	APIKey string `yaml:"api_key" json:"api_key"`
	Model  string `yaml:"model" json:"model"`
}

// Struct que iremos utilizar nos m√©todos, para facilitar as chamadas
type GenAIConfig struct {
	Gemini Gemini `yaml:"gemini" `
	client *genai.Client
	model  *genai.GenerativeModel
	Role   string `yaml:"role" json:"role"`
}

// Interface, no qual √© retornada pela funcaÃÉÃÅo NewGeminiAgent e utilizada dentro de tshot.go
type AgentAI interface {
	Close() error
	Short(ctx context.Context, events *string) (*string, error)
}
```

Agora criaremos o m√©todo de valida√ß√£o a API Key e do model, que ficar√° da seguinte forma:
```
func (gai *GenAIConfig) validate() error {

	if gai == nil || gai.Gemini.APIKey == "" {
		return errors.New("genai config cannot be empty or nil")
	}

// Aqui poderemos ter outro models se for o caso
	switch gai.Gemini.Model {
	case "gemini-2.0-flash":
		gai.Gemini.Model = "gemini-2.0-flash"
	case "gemini-2.0-pro":
		gai.Gemini.Model = "gemini-2.0-pro"
	default:
		gai.Gemini.Model = "gemini-2.0-flash"
	}

	return nil
}
```

Criando a fun√ß√£o NewGeminiAgent, que criar√° a inst√¢ncia da interface
```
// Recebe GenAIConfig, que conter√° a API Key e o model e a role
// Podemos perceber que estamos retornando a interface, que cont√©m Close() e Short()
func NewGeminiAgent(config *GenAIConfig, query *string) (AgentAI, error) {

	gai := config
	var err error
	if err = gai.validate(); err != nil {
		return nil, err
	}

	if query == nil || *query == "" {
		return nil, errors.New("query cannot be empty or nil")
	}

	ctx := context.Background()
	gai.client, err = genai.NewClient(ctx, option.WithAPIKey(gai.Gemini.APIKey))
	if err != nil {
		log.Fatalf("Erro ao criar cliente GenAI: %v", err)
	}

	gai.model = gai.client.GenerativeModel(gai.Gemini.Model)
	gai.Role = *query

	return gai, err
}
```

Criada a fun√ß√£o que inst√¢ncia a LLM, vamos definir o m√©todo Close(), que √© o mais simples desse arquivo at√© agora.
```
func (gai *GenAIConfig) Close() error {
	return gai.client.Close()
}
```

E por √∫timo e o mais importante, a l√≥gica de passar os logs e eventos do kubernetes e fazer a LLM entender o que estamos solicitando
```
// Perceba, que estamos passando os eventos para o m√©todo Short
func (gai *GenAIConfig) Short(ctx context.Context, events *string) (*string, error) {

	if events == nil || *events == "" {
		return nil, errors.New("events cannot be empty or nil")
	}

// iniciando o chart, que foi inicializado o model dentro de NewGeminiAgent
	session := gai.model.StartChat()
  // definindo a role da LLM
	session.History = []*genai.Content{
		{
			Parts: []genai.Part{genai.Text(gai.Role)},
			Role:  "model",
		},
	}

// indicando que o user, est√° enviando os dados a serem analisados
	inputForLLM := fmt.Sprintf("From %s: %s", "user", *events)

// encaminha a strutura da mensagem para LLM
	resp, err := session.SendMessage(ctx, genai.Text(inputForLLM))
	if err != nil {
		return nil, fmt.Errorf("error sending message: %w", err)
	}

// validando se temos a resposta da LLM
	if len(resp.Candidates) == 0 || len(resp.Candidates[0].Content.Parts) == 0 {
		return nil, errors.New("no response from model")
	}

// retornando o conte√∫do gerado pela LLM
	response := fmt.Sprintf("%s", resp.Candidates[0].Content.Parts[0])

	return &response, nil

}

```


Nesse momento, deve estar tudo funcionando


## Executando a CLI
Para executar a CLI e passar os par√¢metros para ela, vamos seguir os seguintes passos:

Acesse o diret√≥rio cmd/cli
```
cd cmd/cli/ 
```

Vamos ver quais as op√ß√µes temos na nossa CLI
```
go run main.go tshot --help
```

Por √∫timo, vamos executar a CLI
```
go run main.go tshot -e "$(kubectl get events -n my-namespace|head -n 10)"
```

Observe as aspas duplas em torno da sa√≠da do comando **kubectl get events**, isso se torna importante, por causa dos espa√ßos.

Ao inv√©s de colocar o **head -n 10**, podemos usar o grep, para pegar algum tipo de reason do erro
```
go run main.go tshot -e "$(kubectl get events -n my-namespace|grep CrashLoopBackOff)"
```

Outra forma de utilizar, √© pegar o describe
```
go run main.go tshot -e "$(kubectl describe pod my-pod|tac|head -n 10)"
```

Nessa sa√≠do, inverti a sa√≠da do arquivo e filtrei as 10 primeiras linhas

## Conclus√£o
Bom, nesse momento, podemos entender um pouco mais do logs e ventos que est√° sendo reportado.

Nos pr√≥ximos passos, iremos criar um chat com sess√£o via CLI e mais adiante, usar o RAG, para aprimorar mais a nossa estrutura